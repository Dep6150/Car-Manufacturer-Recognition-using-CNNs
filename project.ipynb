{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\puig0\\AppData\\Local\\Temp\\ipykernel_32032\\3849256579.py:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Ensuring GPU acceleration is active\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function that is used when loading/processing the image data\n",
    "def preprocess_image(image_path, bbox):\n",
    "    \"\"\"Loads an image, crops it to the bounding box, resizes it, and normalizes it.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess the training data\n",
    "\n",
    "# Load the CSV files\n",
    "df_train = pd.read_csv(\"Data/annotations_train_grouped.csv\")\n",
    "df_grouped = pd.read_csv(\"Data/class_names_manufacturer_country_region_grouped.csv\")\n",
    "\n",
    "# Create a dictionary mapping class indices to class names\n",
    "class_names = {index: row[\"manufacturer\"] for index, row in df_grouped.iterrows()}\n",
    "\n",
    "# File paths to save the preprocessed data\n",
    "x_train_path = \"Data/x_train.npy\"\n",
    "y_train_path = \"Data/y_train.npy\"\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "if os.path.exists(x_train_path) and os.path.exists(y_train_path):\n",
    "    # Load the preprocessed data\n",
    "    x_train = np.load(x_train_path)\n",
    "    y_train = np.load(y_train_path)\n",
    "else:\n",
    "    # Load and preprocess the training data\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i, row in df_train.iterrows():\n",
    "        image_path = f\"Data/cars_train/{row['fname'].split('/')[-1]}\"\n",
    "        bbox = [row['bbox_x1'], row['bbox_y1'], row['bbox_x2'], row['bbox_y2']]\n",
    "        x_train.append(preprocess_image(image_path, bbox))\n",
    "        y_train.append(row[\"class\"])\n",
    "\n",
    "    # Convert the data to NumPy arrays\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    np.save(x_train_path, x_train)\n",
    "    np.save(y_train_path, y_train)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Defining multiple models to explore various approaches\n",
    "\n",
    "def create_model(\n",
    "    num_conv_layers=3,\n",
    "    filters_per_layer=64,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.5,\n",
    "    use_early_stopping=True,\n",
    "):\n",
    "    \"\"\"Creates and compiles a CNN model with the specified parameters.\"\"\"\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(224, 224, 3)))\n",
    "\n",
    "    for i in range(num_conv_layers):\n",
    "        model.add(\n",
    "            layers.Conv2D(\n",
    "                filters_per_layer * 2**i, kernel_size=(3, 3), activation=\"relu\"\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(49, activation=\"softmax\"))\n",
    "\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "# Define the models with different parameters\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"model_1\",\n",
    "        \"num_conv_layers\": 3,\n",
    "        \"filters_per_layer\": 32,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_2\",\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"filters_per_layer\": 32,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_3\",\n",
    "        \"num_conv_layers\": 3,\n",
    "        \"filters_per_layer\": 64,\n",
    "        \"optimizer\": \"rmsprop\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_4\",\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"filters_per_layer\": 64,\n",
    "        \"optimizer\": \"rmsprop\",\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_5\",\n",
    "        \"num_conv_layers\": 3,\n",
    "        \"filters_per_layer\": 32,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_6\",\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"filters_per_layer\": 32,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_7\",\n",
    "        \"num_conv_layers\": 3,\n",
    "        \"filters_per_layer\": 64,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_8\",\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"filters_per_layer\": 64,\n",
    "        \"optimizer\": \"rmsprop\",\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_9\",\n",
    "        \"num_conv_layers\": 5,\n",
    "        \"filters_per_layer\": 32,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"use_early_stopping\": False,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model_10\",\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"filters_per_layer\": 16,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"use_early_stopping\": True,\n",
    "        \"epochs\": 100,\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting to use a pre-trained model\n",
    "base_model = VGG16(\n",
    "    weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze the convolutional base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(49, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=32),\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"car_manufacturer_model_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the model\n",
    "temp = keras.models.load_model(\"car_manufacturer_model_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.save(\"car_manufacturer_model_vgg16.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting to create a single more optimized model without relying on a pre-trained base\n",
    "\n",
    "# More aggressive Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    contrast_stretching=True\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(224, 224, 3)))\n",
    "    for i in range(hp.Int('num_conv_layers', 2, 5)):\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Int('filters_' + str(i), min_value=32, max_value=256, step=32),\n",
    "            kernel_size=hp.Choice('kernel_size_' + str(i), values=[3, 5]),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.3, 0.7, step=0.1)))\n",
    "    model.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner',\n",
    "    project_name='car_classification'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(datagen.flow(x_train, y_train, batch_size=32),\n",
    "             epochs=10,\n",
    "             validation_data=(x_val, y_val))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "# Train the best model with the entire dataset\n",
    "history = best_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=32),\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "model.save(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train models\n",
    "\n",
    "# Train the models\n",
    "for model_params in models:\n",
    "    print(f\"Training {model_params['name']}...\")\n",
    "    model = create_model(\n",
    "        num_conv_layers=model_params[\"num_conv_layers\"],\n",
    "        filters_per_layer=model_params[\"filters_per_layer\"],\n",
    "        optimizer=model_params[\"optimizer\"],\n",
    "        learning_rate=model_params[\"learning_rate\"],\n",
    "        dropout_rate=model_params[\"dropout_rate\"],\n",
    "        use_early_stopping=model_params[\"use_early_stopping\"],\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if model_params[\"use_early_stopping\"]:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "        callbacks.append(early_stopping)\n",
    "\n",
    "    history = model.fit(\n",
    "        datagen.flow(x_train, y_train, batch_size=32),\n",
    "        epochs=model_params[\"epochs\"],\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(f\"{model_params['name']}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model for future use\n",
    "model.save(\"car_manufacturer_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the model\n",
    "model = keras.models.load_model(\"car_manufacturer_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load and preprocess the test data\n",
    "\n",
    "# Load the CSV files\n",
    "df_test = pd.read_csv(\"Data/annotations_test_grouped.csv\")\n",
    "df_grouped = pd.read_csv(\"Data/class_names_manufacturer_country_region_grouped.csv\")\n",
    "\n",
    "# Create a dictionary mapping class indices to class names\n",
    "class_names = {index: row[\"manufacturer\"] for index, row in df_grouped.iterrows()}\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def preprocess_image(image_path, bbox):\n",
    "    \"\"\"Loads an image, crops it to the bounding box, resizes it, and normalizes it.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# File paths to save the preprocessed data\n",
    "x_test_path = \"Data/x_test.npy\"\n",
    "y_test_path = \"Data/y_test.npy\"\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "if os.path.exists(x_test_path) and os.path.exists(y_test_path):\n",
    "    # Load the preprocessed data\n",
    "    x_test = np.load(x_test_path)\n",
    "    y_test = np.load(y_test_path)\n",
    "else:\n",
    "    # process the data\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for i, row in df_test.iterrows():\n",
    "        image_path = f\"Data/cars_test/{row['fname'].split('/')[-1]}\"\n",
    "        bbox = [row['bbox_x1'], row['bbox_y1'], row['bbox_x2'], row['bbox_y2']]\n",
    "        x_test.append(preprocess_image(image_path, bbox))\n",
    "        y_test.append(row[\"class\"])\n",
    "\n",
    "    # Convert the data to NumPy arrays\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    np.save(x_test_path, x_test)\n",
    "    np.save(y_test_path, y_test)\n",
    "    \n",
    "# Create a smaller test set due to an issue when running the full fat dataset for evaluation\n",
    "x_test_small = x_test[:5000]\n",
    "y_test_small = y_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with a smaller subset to debug an issue\n",
    "\n",
    "try:\n",
    "    # Evaluate the model on the smaller test set\n",
    "    loss, accuracy = model.evaluate(x_test_small, y_test_small)\n",
    "    print(\"Test accuracy (small set):\", accuracy)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate all of the models with a slightly reduced size due to kernel crashing when I ran the entire dataset, not sure why and I couldn't fix it\n",
    "model_names = [f\"model_{i}.keras\" for i in range(1, 11)]\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = keras.models.load_model(model_name)\n",
    "        loss, accuracy = model.evaluate(x_test_small, y_test_small)\n",
    "        results.append({\"model_name\": model_name, \"accuracy\": accuracy})\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating {model_name}: {e}\")\n",
    "\n",
    "# Print the results in a formatted table\n",
    "print(\"-\" * 40)\n",
    "print(\"Model\\t\\tAccuracy\")\n",
    "print(\"-\" * 40)\n",
    "for result in results:\n",
    "    print(f\"{result['model_name']}\\t{result['accuracy']:.4f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyze the learned features\n",
    "\n",
    "# Extract the learned features from the last convolutional layer\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=model.inputs, outputs=model.get_layer(\"conv2d_2\").output\n",
    ")\n",
    "features = feature_extractor.predict(x_test)\n",
    "\n",
    "# Flatten the features\n",
    "features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "features_pca = pca.fit_transform(features_flat)\n",
    "\n",
    "# Perform t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000)\n",
    "features_tsne = tsne.fit_transform(features_pca)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(len(class_names)):\n",
    "    plt.scatter(\n",
    "        features_tsne[y_test == i, 0],\n",
    "        features_tsne[y_test == i, 1],\n",
    "        label=class_names[i],\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
